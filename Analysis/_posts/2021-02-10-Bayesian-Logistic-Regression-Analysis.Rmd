---
title: "Bayesian Logistic Regression Analysis"
author: "Aggression to the Mean"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: md_document
category: science
---

```{r setup, include=FALSE} 
# Figure path on disk = base.dir + fig.path
# Figure URL online = base.url + fig.path
knitr::opts_knit$set(base.dir = "/Users/Shared/AttM/", base.url="/AttM/")
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.path = "rmd_images/2021-02-10-bayesian_logistic_regression_analysis/")


# knitr::opts_chunk$set(warning = FALSE, message = FALSE, root.dir = "/Users/Shared/AttM")
# setwd("/Users/Shared/AttM")
```

 <br>
   
## Description 

This script uses inferential statistics to understand UFC fight odds data.   

 <br>
   
### Libraries

```{r}
library(tidyverse)
library(psych)
library(corrplot)
library(knitr)
library(Amelia)
library(rstanarm)
```

 <br>
 
### Examine Data

Load data.
 
```{r}
load("./Datasets/df_master.RData")
```
 
Set the minimum number of fights required for a fighter to be included in the analysis. 

```{r}
fight_min = 1   
```
 
Summarize data.

```{r}
summary(df_master)
```

Redefine variables. 

```{r}
df_master$NAME = as.factor(df_master$NAME)
df_master$Date = as.Date(df_master$Date)
df_master$Event = as.factor(df_master$Event)
df_master$City= as.factor(df_master$City)
df_master$State = as.factor(df_master$State)
df_master$Country = as.factor(df_master$Country)
df_master$FightWeightClass = as.factor(df_master$FightWeightClass)
# will keep round as integer since represents time in match...
# df_master$Round = as.factor(df_master$Round)  
df_master$Method = as.factor(df_master$Method)
df_master$Winner_Odds = as.numeric(df_master$Winner_Odds)
df_master$Loser_Odds = as.numeric(df_master$Loser_Odds)
df_master$fight_id = as.factor(df_master$fight_id)
df_master$Sex = as.factor(df_master$Sex)
df_master$Result = as.factor(df_master$Result)
df_master$FighterWeightClass = as.factor(df_master$FighterWeightClass)
```

Summarize again. There are infinite odds and overturned / DQ fight outcomes. These will have to be removed. 

```{r}
summary(df_master)
```

How many events are there in the dataset?

```{r}
length(unique(df_master$Event))
```

How many fights? 

```{r}
length(unique(df_master$fight_id))
```

Over what time frame did these occur? 

```{r}
range(sort(unique(df_master$Date)))
```

 <br>

### Preprocess Data

Make copy of data frame. 

```{r}
df_stats = df_master
```

Calculate number of fights in dataset for each fighter in dataset. 

```{r}
df_stats %>%
  dplyr::group_by(NAME) %>%
  dplyr::summarise(
    Num_Fights = length(Round)
  ) -> df_fight_count
```

Append fight count to dataframe. 

```{r}
df_stats = merge(df_stats, df_fight_count)
```

Which fights will we lose due to equal odds? 

```{r}
df_stats %>%
  dplyr::filter(Winner_Odds == Loser_Odds) -> df_equal_odds

kable(df_equal_odds)

```

Filter out controversial results (DQ and Overturned) and equal odds.  

```{r}
df_stats %>%
  dplyr::filter(
    (Method != "DQ") & (Method != "Overturned")
    , Winner_Odds != Loser_Odds
    , Num_Fights >= fight_min
  ) -> df_stats
```

How many rows do we lose? 

```{r}
nrow(df_master) - nrow(df_stats)
```

Also convert infinite odds to NAs. 

```{r}
df_stats %>%
  dplyr::mutate(
    Winner_Odds = ifelse(is.infinite(Winner_Odds), NA, Winner_Odds)
    , Loser_Odds = ifelse(is.infinite(Loser_Odds), NA, Loser_Odds)
  ) -> df_stats
```

Get rid of lonely fight ids (i.e. instances where one of the two competitiors was removed from the dataset). 

```{r}
df_stats %>%
  dplyr::group_by(fight_id) %>%
  dplyr::summarise(Count = length(NAME)) %>%
  dplyr::filter(Count != 2) -> lonely_ids

idx_lonely_ids = lonely_ids$fight_id

df_stats = df_stats[!(df_stats$fight_id %in% idx_lonely_ids), ]
```

How many additional rows do we lose? 

```{r}
length(idx_lonely_ids)
```

What percentage of fights, from those that were succesfully scrapped, do we keep? 

```{r}
length(unique(df_stats$fight_id)) / length(unique(df_master$fight_id)) 
```

How many fights are we left with? 

```{r}
length(unique(df_stats$fight_id))
```

Over what period of time? 

```{r}
range(df_stats$Date)
```

How many fights per Year?

```{r}
df_stats %>%
  dplyr::mutate(
    Year = as.numeric(format(Date,"%Y"), ordered = T)
  ) %>%
  dplyr::group_by(Year) %>%
  dplyr::summarise(
    count = length(unique(fight_id))
  ) -> df_year_count

kable(df_year_count)
```

Create additional columns to frame everything vis-a-vis favorite. 

```{r}
df_stats %>%
  dplyr::mutate(
    Favorite_Won = ifelse(Winner_Odds < Loser_Odds, T, F)
    , Was_Favorite = ifelse(
      (Result == "Winner" & Favorite_Won)|(Result == "Loser" & !Favorite_Won)
      , T
      , F
      )
  ) -> df_stats
```

Get odds and transform probability to logit space. 

```{r}
df_stats %>%
  dplyr::mutate(
    fighter_odds = ifelse(Result == "Winner", Winner_Odds, Loser_Odds)
    , implied_prob = 1/fighter_odds
    , logit_prob = qlogis(1/fighter_odds)
    , Year = as.numeric(format(Date,"%Y"))
  ) -> df_stats
```

 <br>

### Visualize Data

Many of the stats are repeated several times due to fighters having several fights. Ideally we would account for this but for the purpose of visualizing the data to get an idea of what we are dealing with, we won't bother.   

Later on, the predictors will end up being the difference in stats between fighters. Since most fights are not re-matches, there will not be a major concern.  

```{r}
df_stats %>%
  dplyr::select(
    fight_id
    , Sex
    , Favorite_Won
    , Was_Favorite
    , Result
    , Year
    , REACH
    , implied_prob
    , logit_prob
  ) -> df_for_graph
```

Examine correlations among potential predictors. There do not appear to be any notable correlations. 

```{r}
df_cor = cor(df_for_graph[6:9], method = c("spearman"), use = "na.or.complete")
corrplot(df_cor)
df_cor
```

Visualize the relationship between the Implied Probabilities of the odds in original and transformed space. The graph also gives a sense of the distribution of the Implied Probabilities and the number of them which start racing towards to edges of transformed space (i.e. distortions close to the limits of the original scale).  

```{r}
df_for_graph %>%
  ggplot(aes(x=implied_prob, y=logit_prob))+
  geom_point()+
  geom_smooth(se = F, method = "lm")+
  ylab("Logit Implied Probability")+
  xlab("Implied Probability")
```

Create function to visualize data as boxplots. 

```{r}
boxplot_df_for_graph = function(df = df_for_graph, grouping = "Was_Favorite") {
  df$Grouping = df[,which(colnames(df) == grouping)]
  
  df %>%
    gather(key = "Metric", value = "Value", REACH:logit_prob) %>%
    ggplot(aes(x=Grouping, y=Value, group = Grouping, color = Grouping))+
    geom_boxplot()+
    labs(color = grouping) +
    xlab(grouping)+
    facet_wrap(.~Metric, scales = "free", nrow = 2) -> gg
  print(gg)
  
}
```

Examine distribution of predictors as a function of which fighter was the favorite.  

```{r}
boxplot_df_for_graph()
```

Examine distribution of predictors as a function of who won.  

```{r}
boxplot_df_for_graph(grouping = "Result")
```

Examine distribution of predictors as a function of Sex.   

```{r}
boxplot_df_for_graph(grouping = "Sex")
```

Examine distribution of predictors as a function of Year.   

```{r}
boxplot_df_for_graph(grouping = "Year")
```

Examine distribution of potential predictors. Of course, the distributions of Implied Probabilities are missing their peaks due to the removal of fights with equal odds.   

Otherwise, Reach is somewhat normally distributed.   

```{r}
df_for_graph %>%
  gather(key = "Metric", value = "Value", Year:logit_prob) %>%
  ggplot(aes(x=Value))+
  geom_histogram()+
  facet_wrap(.~Metric, scales = "free", nrow = 3)
```

 <br>

### Compute Differences between Fighters

Create separate data frames for favorites and underdogs, then merge them. 

```{r}
# Favorites
df_stats %>%
  dplyr::filter(Was_Favorite) %>%
  dplyr::select(
    fight_id
    , Sex
    , Favorite_Won
    , Year
    , REACH
    , implied_prob 
    , logit_prob
    ) -> df_favs

# Underdogs
df_stats %>%
  dplyr::filter(!Was_Favorite) %>%
  dplyr::select(
    fight_id
    , Sex
    , Favorite_Won
    , Year
    , REACH
    , implied_prob
    , logit_prob
  ) -> df_under
# rename
df_under %>%
  rename(
    U_REACH = REACH
    , U_implied_prob = implied_prob
    , U_logit_prob = logit_prob
  ) -> df_under

# Merge
df_both = merge(df_under, df_favs)
```

Examine new dataframe.

```{r}
summary(df_both)
```

How many fights do we have? 

```{r}
nrow(df_both)
```

How often does the favorite win? 

```{r}
mean(df_both$Favorite_Won)
```

Compute differences in Reach. Also, adjust for the overround. 

```{r}
df_both %>%
  dplyr::group_by(fight_id) %>%
  dplyr::summarise(
    Favorite_Won=Favorite_Won
    , Sex=Sex
    , Year=Year
    , Delta_REACH = REACH - U_REACH
    , Log_Odds = logit_prob
    , Implied_Prob = implied_prob
    , Adjust_Implied_Prob = implied_prob - (implied_prob + U_implied_prob - 1)/2
  ) -> df_og_diff
```

Get Adjusted Log Odds.  

```{r}
df_og_diff %>% 
  mutate(Adjust_Log_Odds = qlogis(Adjust_Implied_Prob)) -> df_og_diff
```


Examine new dataframe. Notice that Adjusted Implied Probabilities never go under 0.5. Similarly, Adjusted Log Odds never go below 0. 

```{r}
summary(df_og_diff)
```

Examine correlations among potential predictors.  

```{r}
df_cor_diff = cor(df_og_diff[5:9], method = c("spearman"), use = "na.or.complete")
corrplot(df_cor_diff)
df_cor_diff
```

Visualize the relationship between Log Odds and Adjusted Log Odds. The red line is y=x whereas the blue line is the line of best fit.  

The adjustments do not appear to vary with respect to Log Odds (i.e. closer contests did not tend to have more overround than more disperate ones, etc.).    

```{r}
df_og_diff %>%
  ggplot(aes(x=Log_Odds, y=Adjust_Log_Odds))+
  geom_point()+
  geom_smooth(se = F, method = "lm")+
  geom_abline(slope=1, color = "red")+
  ylab("Adjusted Log Odds")+
  xlab("Log_Odds")
```

Plot similar graph for Implied Probabilities. 

```{r}
df_og_diff %>%
  ggplot(aes(x=Implied_Prob, y=Adjust_Implied_Prob))+
  geom_point()+
  geom_smooth(se = F, method = "lm")+
  geom_abline(slope=1, color = "red")+
  ylab("Adjusted Implied Probability")+
  xlab("Implied Probability")
```

 <br>

### Deal with Missing Values

```{r}
missing_reach <- round(mean(is.na(df_og_diff$Delta_REACH))*100)
```

How many Reach entries are missing? Around `r missing_reach`%.  

```{r}
df_og_diff %>%
  select("Delta_REACH", "fight_id") -> df_for_amelia

missmap(df_for_amelia)
```

Is there a relationship between missingness of Reach and actual values of Reach? Based on the random sample below, fighters with no recorded Reach appear to not be popular overall. Therefore, they are likely NOT strong competitors on average. This will be something to keep in mind as it could introduce some kind of bias into the dataset / results.  

```{r}
df_master %>%
  dplyr::filter(is.na(REACH)) %>%
  dplyr::select(
  "Date"
  , "NAME"
  , "FightWeightClass"
  ) -> df_reach_nas 

kable(df_reach_nas[sample(1:nrow(df_reach_nas), nrow(df_reach_nas)/10),])
```

Compare the summaries of the subset of data without NAs to the one with NAs to identify notable differences.   

```{r}
df_og_diff %>%
  dplyr::filter(is.na(Delta_REACH)) -> df_nas

df_og_diff %>%
  dplyr::filter(!is.na(Delta_REACH)) -> df_nonas

summary(df_nas)
summary(df_nonas)
```

Favorites appear to be more likely to win when Reach is NA despite not being substantially more favored (see above).

```{r}
mean(df_nas$Favorite_Won) - mean(df_nonas$Favorite_Won)
```

Using original dataset, look at if those without Reach entry tend to be favored or not. Indeed they tended to be the underdogs. Therefore, it seems like those with Reach NAs tend to underperform relative to their odds, when considering the above. As such, removing entries with missing Reach data in a future analysis could affect the results. 

```{r}
df_stats %>%
  dplyr::filter(is.na(REACH)) -> df_nas_odds 

mean(df_nas_odds$implied_prob)
mean(df_nas_odds$Was_Favorite)
```

Also, the fights with Reach NAs are a couple years older on average. This may be due to improved stats collection and tracking through the years. 

```{r}
mean(df_nas$Year) - mean(df_nonas$Year)
```
 
I may consider a simple random imputation for the model with both Reach and Log Odds as predictors, to avoid losing cases with higher rates of underperformance.   

 <br>

### Visualize Difference Data  

Create function to generate boxplots of difference data.  

```{r}
# function for box plot
boxplot_df_og_diff = function(df = df_og_diff, grouping = NULL, do_result = F) {
  
  if (is.null(grouping)) {
    
    if (do_result) {
      df %>%
        gather(key = "Metric", value = "Value", Delta_REACH:Adjust_Log_Odds) %>%
        dplyr::mutate(Value = ifelse(Favorite_Won, Value, -Value)) %>%  # THIS FLIPS SIGN 
        ggplot(aes(x=Metric, y=Value))+
        geom_boxplot()+
        facet_wrap(.~Metric, scales = "free", nrow = 2)+
        ggtitle("Winner - Loser") -> gg
    } else {
      df %>%
        gather(key = "Metric", value = "Value", Delta_REACH:Adjust_Log_Odds) %>%
        ggplot(aes(x=Metric, y=Value))+
        geom_boxplot()+
        facet_wrap(.~Metric, scales = "free", nrow = 2)+
        ggtitle("Favorite - Underdog") -> gg
    }
    

    
  } else {
    
    df$Grouping = df[,which(colnames(df) == grouping)][[1]]
    
    df %>%
      gather(key = "Metric", value = "Value", Delta_REACH:Adjust_Log_Odds) %>%
      ggplot(aes(x=Grouping, y=Value, group = Grouping, color = Grouping))+
      geom_boxplot()+
      labs(color = grouping) +
      xlab(grouping)+
      ggtitle("Favorite - Underdog")+
      facet_wrap(.~Metric, scales = "free", nrow = 2) -> gg
  }
  
  print(gg)
  
}


```

Generate boxplots for potential predictors. I am including all versions of Implied Probability/Log Odds to compare them. Of course, I will only include one of these as a predictor in the model.  

```{r}
boxplot_df_og_diff()
```

Compare predictors when the Favorite wins and loses.  

```{r}
boxplot_df_og_diff(grouping = "Favorite_Won")
```

Compare predictors as a function of Sex.  

```{r}
boxplot_df_og_diff(grouping = "Sex")
```

Compare predictors as a function of Year.  

```{r}
boxplot_df_og_diff(grouping = "Year")
```

Modify predictors to look at difference in stats between Winner and Loser (instead of Favorite and Underdog).   

```{r}
boxplot_df_og_diff(do_result = T)
```

 <br>

### Relationship between Predictors and Outcome 

Create function to plot Outcome as a function of Predictors. 

```{r}
# function
plot_against_log_odds = function(df = df_og_diff, variable = "Log_Odds", pred_log_odds = F, num_bin = 20, min_bin_size = 30) {
  
  # create dummy variable for function
  df$Dummy = df[
    ,which(colnames(df) == sprintf("%s", variable))
  ][[1]]
  
  # as numeric
  df$Dummy = as.numeric(df$Dummy)
  
  # get bins
  df$Dummy_Bin = cut(df$Dummy, num_bin)
  
  
  # get log odds of Favorite victory by bin
  df %>%
    dplyr::group_by(Dummy_Bin) %>%
    dplyr::summarise(
      Prop_of_Victory = mean(Favorite_Won)
      , Log_Odds_Victory = logit(Prop_of_Victory)
      , Size_of_Bin = length(Favorite_Won)
      , Dummy = mean(Dummy)
    ) -> fav_perf
  
  # extract bins
  fav_labs <- as.character(fav_perf$Dummy_Bin) 
  fav_bins = as.data.frame(
    cbind(
      lower = as.numeric( sub("\\((.+),.*", "\\1", fav_labs) )
      , upper = as.numeric( sub("[^,]*,([^]]*)\\]", "\\1", fav_labs) )
    )
  )
  # get value in middle of bin
  fav_bins %>% dplyr::mutate(mid_bin = (lower + upper)/2 ) -> fav_bins
  # add mid bin column
  fav_perf$Mid_Bin = fav_bins$mid_bin
  
  if (pred_log_odds) {
    fav_perf %>%
    dplyr::filter(Size_of_Bin >= min_bin_size) %>%
    ggplot(aes(x=Dummy, y=Log_Odds_Victory))+
    geom_point()+
    geom_smooth(se=F , method="lm")+
    geom_abline(slope = 1, color = "red")+
    # geom_smooth()+
    ylab("Log Odds that Favorite Won")+
    xlab(sprintf("Mean %s", variable))->gg
  print(gg)
  } else {
      fav_perf %>%
      dplyr::filter(Size_of_Bin >= min_bin_size) %>%
      ggplot(aes(x=Dummy, y=Log_Odds_Victory))+
      geom_point()+
      geom_smooth(se=F , method="lm")+
      # geom_smooth()+
      ylab("Log Odds that Favorite Won")+
      xlab(sprintf("Mean %s", variable))->gg
    print(gg)
  }

  
}
```

Unsurprisingly, the log odds of the implied probabilities are good linear predictors of the actual log odds of victory. Importantly, the Adjusted Log Odds appear to outperform the non-adjusted Log Odds.  

```{r}
plot_against_log_odds(pred_log_odds = T)
plot_against_log_odds(variable = "Adjust_Log_Odds", pred_log_odds = T)
```

The linear fit is not quite as nice using the Implied Probability metrics. With that said, Implied Probability could likely still be used very succesfully as a predictor - the fit is still solid. Theoretically, we would have expected issues at the limits of Implied Probability (90%+). However, in practice, Implied Probabilities seldom reach those limits. 

```{r}
plot_against_log_odds(variable = "Implied_Prob")
plot_against_log_odds(variable = "Adjust_Implied_Prob")
```

Look at y axis scale. There is virtually no difference in Log Odds of victory between sexes.  

```{r}
plot_against_log_odds(variable = "Sex")
```

Favorites may be less likely to win in recent years. However, this may simply be an artifact of an unstable estimate for year 2013. (That point appears to have a lot of leverage.)  

```{r}
plot_against_log_odds(variable = "Year") 
```

There may be a positive effect of Reach on Log Odds of victory.  

```{r}
plot_against_log_odds(variable = "Delta_REACH") 
```

 <br>

### Assumptions

Here are the [assumptions of logistic regression](https://www.statisticssolutions.com/assumptions-of-logistic-regression):  
- First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.  
- Second, logistic regression requires the observations to be independent of each other.  In other words, the observations should not come from repeated measurements or matched data.  
- Third, logistic regression requires there to be little or no multicollinearity among the independent variables.  This means that the independent variables should not be too highly correlated with each other.  
- Fourth, logistic regression assumes linearity of independent variables and log odds.  although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.  

In our case, (1) is met; (2) should be met, however, there could be random effects of Fighter or Event, etc. which likely would not impact prediction substantially but could influence explanatory analysis; (3) seems satisfactorily met (there are no major correlations); and (4) appears to be met (regardless of whether or not we use Log Odds or Implied Probability).  

 <br>

### Adjusted Log Odds of Implied Probability as Single Predictor

```{r}
fit_1 <- stan_glm(Favorite_Won ~ Adjust_Log_Odds, family=binomial(link="logit"), data = df_og_diff)
print(fit_1)
```

Display uncertainty in the parameters. 

```{r}
sims_1 <- as.matrix(fit_1)
n_sims <- nrow(sims_1)

draws_1 <- sample(n_sims, 20)

curve(invlogit(sims_1[draws_1[1],1] + sims_1[draws_1[1],2]*x)
      , from = 0
      , to = 3
      , col = "gray"
      , lwd=0.5
      , xlab="Adjusted Log Odds of Favorite"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_1[2:20]) {
  curve(invlogit(sims_1[j,1] + sims_1[j,2]*x)
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}
```

Now do the same with the backtransformed predictors (i.e. Adjusted Log Odds to Implied Probabilities). Here we can see that the negative intercept allows the model to capture the overperformance of large favorites and the underperformance of mild ones. 

```{r}

curve(invlogit(sims_1[draws_1[1],1] + sims_1[draws_1[1],2]*logit(x))
      , from = 0.5
      , to = 1
      , col = "gray"
      , lwd=0.5
      , xlab="Adjusted Implied Probability of Favorite"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_1[2:20]) {
  curve(invlogit(sims_1[j,1] + sims_1[j,2]*logit(x))
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}
abline(a=0, b=1, col = "red")
```


Evaluate intercept (i.e. with Adjusted Log Odds equal to zero). Note: Implied Probability of 50% is technically right outside the range of the data.  

```{r}
b1 <- fit_1$coefficients[[1]]
b2 <- fit_1$coefficients[[2]]
invlogit(0)
invlogit(b1 + b2 * 0)
```

Now, with Adjusted Log Odds of 1/2 which is equivalent to about 62% Adjusted Implied Probability. 

```{r}
invlogit(0.5)
invlogit(b1 + b2 * 0.5)
```

Now, with Adjusted Log Odds of 1 which is equivalent to about 73% Adjusted Implied Probability. 

```{r}
invlogit(1)
invlogit(b1 + b2 * 1)
```

Now, with Adjusted Log Odds of 3/2 which is equivalent to about 82% Adjusted Implied Probability. 

```{r}
invlogit(3/2)
invlogit(b1 + b2 * 3/2)
```

With the intercept, the model captures the fact that mild favorites underperform whereas large favorites overperform.   

Now, we'll try the divide-by-4-rule.  Indeed, the actual change in the Probability of the Favorite Winning is just under the coefficient divided by 4. 

```{r}
b2/4
invlogit(b1 + b2 * 1) - invlogit(b1 + b2 * 0)
```

Get point predictions using predict() and compare to Adjusted Log Odds to again get a sense of how model manages to account for underperformances and overperformances.   

```{r}
newx = seq(0, 3, 0.5)
new <- data.frame(Adjust_Log_Odds=newx)
pred <- predict(fit_1, type = "response", newdata = new)

new$BackTrans_Implied_Prob <- invlogit(newx)
new$Point_Pred <- pred
```

Get expected outcome with uncertainty.  

```{r}
epred <- posterior_epred(fit_1, newdata=new)

new$Means <- apply(epred, 2, mean)
new$SDs <- apply(epred, 2, sd)
```

Predictive distribution for new observation.  Taking the mean of the predictions gives us similar results as above.  

```{r}
postpred <- posterior_predict(fit_1, newdata=new)

new$Mean_Pred <- apply(postpred, 2, mean)
kable(new)
```

Calculate log score for pure chance. 

```{r}
logscore_chance <- log(0.5) * length(fit_1$fitted.values)
logscore_chance
```

Calculate log score for simply following adjusted best odds. 

```{r}
y <- fit_1$data$Favorite_Won
x <- fit_1$data$Adjust_Implied_Prob
logscore_bestodds <-  sum(y * log(x) + (1-y)*log(1-x))
logscore_bestodds
```

Calculate log score for model.  

```{r}
predp_1 <- predict(fit_1, type = "response")
logscore_1 <- sum(y * log(predp_1) + (1-y)*log(1-predp_1))
logscore_1
```

Run leave one out cross validation. There is about a two point difference between the elpd_loo estimate and the within-sample log score which makes sense since the fitted model has two parameters.   

```{r}
loo_1 <- loo(fit_1)
print(loo_1)
```

<br>

### Adjusted Implied Probability as Single Predictor

```{r}
fit_2 <- stan_glm(Favorite_Won ~ Adjust_Implied_Prob, family=binomial(link="logit"), data = df_og_diff)
print(fit_2)
```

Display uncertainty in the parameters. 

```{r}
sims_2 <- as.matrix(fit_2)
n_sims_2 <- nrow(sims_2)

draws_2 <- sample(n_sims_2, 20)

curve(invlogit(sims_2[draws_2[1],1] + sims_2[draws_2[1],2]*x)
      , from = 0.5
      , to = 1
      , col = "gray"
      , lwd=0.5
      , xlab="Adjusted Implied Probability of Favorite"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_2[2:20]) {
  curve(invlogit(sims_2[j,1] + sims_2[j,2]*x)
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}
abline(a=0, b=1, col = "red")
```

Get point predictions using predict() and compare to Adjusted Implied Probabilities to get a sense of how model manages to account for underperformances and overperformances. 

```{r}
newx_2 = seq(0.5, 1, 0.1)
new_2 <- data.frame(Adjust_Implied_Prob=newx_2)
new_2$Point_Pred <- predict(fit_2, type = "response", newdata = new_2)
```

Get expected outcome with uncertainty.    

```{r}
epred_2 <- posterior_epred(fit_2, newdata=new_2)

new_2$Means <- apply(epred_2, 2, mean)
new_2$SDs <- apply(epred_2, 2, sd)
```

Predictive distribution for new observation.  Taking the mean of the predictions gives us similar results as above.  

```{r}
postpred_2 <- posterior_predict(fit_2, newdata=new_2)

new_2$Mean_Pred <- apply(postpred_2, 2, mean)
kable(new_2)
```

Calculate log score for second model (i.e. with Implied Probabilities). The log score is marginally worst than the one using the Adjusted Log Odds (`r round(logscore_1,2)`).   

```{r}
predp_2 <- predict(fit_2, type = "response")
logscore_2 <- sum(y * log(predp_2) + (1-y)*log(1-predp_2))
logscore_2
```

Run leave one out cross validation.  Similarly, the elpd_loo is marginally worst than the one using the Adjusted Adjusted Log Odds (`r round(loo_1$estimates[[1,1]],2)`).   

```{r}
loo_2 <- loo(fit_2)
print(loo_2)
```

The difference between using the Adjusted Log Odds or Implied Probabilities as a predictor is fairly small. However, using the Adjsuted Log Odds makes more sense in principle and leads to a marginally better performance. Also, it is trivially easy to back-transform the predictors for interpretability.  Therefore, we will use Adjusted Log Odds.  

<br>

### Adjusted Log Odds with Square Term.  

```{r}
df_og_diff$Adjust_Log_Odds_sq = (df_og_diff$Adjust_Log_Odds)^2
fit_3 <- stan_glm(Favorite_Won ~ Adjust_Log_Odds + Adjust_Log_Odds_sq, family=binomial(link="logit"), data = df_og_diff)
print(fit_3)
```

Display uncertainty in the parameters. Take 40 draws to capture strange behavior of some of the draws on implied probability scale.  

```{r}
sims_3 <- as.matrix(fit_3)
n_sims_3 <- nrow(sims_3)

draws_3 <- sample(n_sims_3, 40)

curve(invlogit(sims_3[draws_3[1],1] + sims_3[draws_3[1],2]*x + sims_3[draws_3[1],3]*(x^2))
      , from = 0
      , to = 3
      , col = "gray"
      , lwd=0.5
      , xlab="Adjusted Log Odds of Favorite"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_3[2:40]) {
  curve(invlogit(sims_3[j,1] + sims_3[j,2]*x + sims_3[j,3]*(x^2))
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}
```

Now do the same with the backtransformed predictors (i.e. Adjusted Log Odds to Implied Probabilities).  Some of the draws exhibit strange behavior toward the limit of the implied probabilities (i.e. close to 1) whereby they dip dramatically towards lower outcome values.  

```{r}
curve(invlogit(sims_3[draws_3[1],1] + sims_3[draws_3[1],2]*logit(x) + sims_3[draws_3[1],3]*(logit(x)^2))
      , from = 0.5
      , to = 1
      , col = "gray"
      , lwd=0.5
      , xlab="Adjusted Implied Probability of Favorite"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_3[2:40]) {
  curve(invlogit(sims_3[j,1] + sims_3[j,2]*logit(x) + sims_3[j,3]*(logit(x)^2))
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}

abline(a=0, b=1, col = "red")
```

Run leave one out cross validation.  The model with the square terms performs about as well as the model with the Adjusted Implied Probabilities. Therefore, we will stick with the basic single predictor model using Adjusted Log Odds as a predictor.  

```{r}
loo_3 <- loo(fit_3)
print(loo_3)
```
<br>

### Reach as Lone Predictor  

We run the model with just Reach as a predictor. There are missing Reach values that we have not replaced. Indeed, the number of observations is reduced compared to the models above.  

```{r}
fit_4 <- stan_glm(Favorite_Won ~ Delta_REACH, family=binomial(link="logit"), data = df_og_diff)
print(fit_4)
```

Need to see additional decimal places for coefficients and SEs.  It looks like there is a positive effect of Reach such that fighters with longer reaches than their opponents have a greater probability of victory than those with shorter reaches than their opponents.  

```{r}
fit_4$coefficients
fit_4$ses
```

Display uncertainty in the parameters. The below graphs gives a sense of the uncertainty as well as the magnitude of the effect. The effect of Reach absent any other information does appear to be positive.  

```{r}
sims_4 <- as.matrix(fit_4)
n_sims_4 <- nrow(sims_4)



draws_4 <- sample(n_sims_4, 20)

curve(invlogit(sims_4[draws_4[1],1] + sims_4[draws_4[1],2]*x)
      , from = -15
      , to = 15
      , col = "gray"
      , lwd=0.5
      , xlab="Difference in Reach (Favorite - Underdog; inches)"
      , ylab = "Probability of Favorite Winning"
      , main = "Random Draws from Parameter Simulations"
      )

for (j in draws_4[2:20]) {
  curve(invlogit(sims_4[j,1] + sims_4[j,2]*x)
        , col = "gray"
        , lwd=0.5
        , add=TRUE
        )
}
```


Evaluate intercept (i.e. with Difference in Reach equal to zero).  This should approximately represent the average Probability of Victory of the favorite, although not exactly since Favorites have slightly greater reaches than their opponents on average (a little over a quarter inch).  

```{r}
b1_4 <- fit_4$coefficients[[1]]
b2_4 <- fit_4$coefficients[[2]]
invlogit(b1_4 + b2_4 * 0)
mean(fit_4$data$Delta_REACH, na.rm = T)
```

Now, we'll try the divide by 4 rule for reach differences ranging from 1 to 5 inches.  

```{r}
b2_4/4 * c(1:5)

b2_4_5 <- round(b2_4/4 * 5 * 100)
b2_4_1 <- round(b2_4/4 * 1 * 100, 1)
```

It looks like those with a 5 inch reach advantage may have about a `r b2_4_5`% greater probability of victory than those with no reach advantage.  Indeed, each one inch reach advantage corresponds to approximately a `r b2_4_1`% increase in probability of victory.   

Get point predictions using predict(). As with the divide-by-4-rule, a 5 inch Reach advantage is associated with almost a 4% increase with the Probability of the Favorite Winning.   

```{r}
newx_4 = seq(-10, 10, 1)
new_4 <- data.frame(Delta_REACH=newx_4)
new_4$Point_Pred <- predict(fit_4, type = "response", newdata = new_4)

min_point_pred <- round(min(new_4$Point_Pred)*100)
max_point_pred <- round(max(new_4$Point_Pred)*100)
diff_point_pred <- max_point_pred - min_point_pred 
```

With the most extreme comparison, from -10 Reach to +10 Difference in Reach, the Probability of Victory jumps from around `r min_point_pred`% to `r max_point_pred`% - that's a `r diff_point_pred`% difference.  However, as we could see from the parameter draws, there is likely a lot of uncertainty about those more extreme estimates.  

Get expected outcome with uncertainty. The uncertainty in the estimate increases with larger Reach Difference values, which was expected. However, for Reach Difference within 5 inches, the SDs for Probability of Victory are below 2%.  

```{r}
epred_4 <- posterior_epred(fit_4, newdata=new_4)
new_4$Means <- apply(epred_4, 2, mean)
new_4$SDs <- apply(epred_4, 2, sd)
```

Predictive distribution for new observation.  Taking the mean of the predictions gives us similar results as above.  

```{r}
postpred_4 <- posterior_predict(fit_4, newdata=new_4)
new_4$Mean_Pred <- apply(postpred_4, 2, mean)
kable(new_4)
```

Calculate log score for pure chance. 

```{r}
logscore_chance_4 <- log(0.5) * length(fit_4$fitted.values)
logscore_chance_4  
```

Calculate log score for model. The simple model with Reach clearly outperforms chance.  

```{r}
y_4 <- fit_4$model$Favorite_Won
predp_4 <- predict(fit_4, type = "response")
logscore_4 <- sum(y_4 * log(predp_4) + (1-y_4)*log(1-predp_4))
logscore_4
```

However, does the model outperform picking the favorite at the base rate of victory (i.e. intercept only)?     

```{r}
base_rate_4 <- mean(fit_4$model$Favorite_Won)
logscore_inter_4 <- sum(y_4 * log(base_rate_4) + (1-y_4)*log(1-base_rate_4))
logscore_inter_4

score_diff_inter_4 <- round(logscore_4 - logscore_inter_4,2)
```

It does, but not by much - only by a logscore of `r score_diff_inter_4`, which means adding Reach as a factor probably minimally improves the predictions.  

Run leave one out cross validation.  We don't have another model to compare it to but we can see that elpd_loo is is about two points more negative than the logscore calculation which is not surprising given that the model has two parameters.  

```{r}
loo_4 <- loo(fit_4)
print(loo_4)
```
<br>

### Impute Missing Reach Values for Two Predictor Model

Do simple random imputation.

We want to avoid removing cases with missing Reach values since, as we saw earlier, those cases overrepresent underperformances and underdogs. Therefore, if we were to remove them, we may bias the results and affect the relationship between Adjusted Fight Odds and the outcome variable (Probability of Favorite Winning).    

NOTE: With a more complex model, with multiple predictors and missing values across them, we may consider multiple imputation.   

Based on the summary below, we see that the imputed values of Difference in Reach are virtually the same as the original ones (i.e. with NAs excluded). This is as expected, especially since we only replaced approximately `r missing_reach`% of Reach entries (NAs).   

```{r}
random_imp <- function(a) {
  missing <- is.na(a)
  n_missing <- sum(missing)
  a_obs <- a[!missing]
  imputed <- a
  imputed[missing] <- sample(a_obs, n_missing)
  
  return(imputed)
}

df_og_diff$Delta_REACH_imp <- random_imp(df_og_diff$Delta_REACH)

summary(df_og_diff)
```

<br>

### Two Predictor Model: Imputed Reach and Adjusted Log Odds

```{r}
fit_5 <- stan_glm(Favorite_Won ~ Adjust_Log_Odds + Delta_REACH_imp
                  , family=binomial(link="logit")
                  , data = df_og_diff
                  )
print(fit_5)
```

Get better sense of coefficients. Unsurprisingly, Adjusted Log Odds still has a large effect. The (imputed) Difference in Reach still seems to have an effect. The magnitude of the effect decreased a bit but the standard error stayed about the same (compared to the model with just Reach).  

```{r}
fit_5$coefficients
fit_5$ses
```


Display uncertainty in the parameters. The graph below plots random draws from the parameter simulations for 5 values of Difference in Reach: -10 (green), -5 inches (red), 0 inches (grey), +5 inches (blue), and +10 (yellow). We see the Difference in Reach sort of just shifts the curves (and therefore the Probability of the Favorite Winning) upwards. 

```{r}
sims_5 <- as.matrix(fit_5)
n_sims_5 <- nrow(sims_5)

draws_5 <- sample(n_sims_5, 20)

curve(invlogit(sims_5[draws_5[1],1] + sims_5[draws_5[1],2]*x + sims_5[draws_5[1],3]*(0))
    , from = 0
    , to = 3
    , col = "black"
    , lwd=0.0       # make invisible, just used to set up plot. 
    , xlab="Adjusted Log Odds of Favorite"
    , ylab = "Probability of Favorite Winning"
    , main = "Random Draws from Parameter Simulations"
    )

for (j in draws_5) {
  
   curve(invlogit(sims_5[j,1] + sims_5[j,2]*x + sims_5[j,3]*(-10))
        , col = "green"
        , lwd=0.3
        , add=TRUE
        )
  
    curve(invlogit(sims_5[j,1] + sims_5[j,2]*x + sims_5[j,3]*(-5))
        , col = "red"
        , lwd=0.3
        , add=TRUE
        )
  
    curve(invlogit(sims_5[j,1] + sims_5[j,2]*x + sims_5[j,3]*(0))
        , col = "black"
        , lwd=0.3
        , add=TRUE
        )
    
      curve(invlogit(sims_5[j,1] + sims_5[j,2]*x + sims_5[j,3]*(+5))
        , col = "blue"
        , lwd=0.3
        , add=TRUE
        )
      
      curve(invlogit(sims_5[j,1] + sims_5[j,2]*x + sims_5[j,3]*(+10))
        , col = "yellow"
        , lwd=0.3
        , add=TRUE
        )
}
```

Now do the same with the backtransformed predictors (i.e. Adjusted Log Odds to Implied Probabilities). It appears that when there is no Reach Difference (i.e. black lines), the model account for some of the underperformance of mild favorites / overperformance of large favorites. Otherwise, Reach Difference simply results in a shift in the curve.  

```{r}
curve(invlogit(sims_5[draws_5[1],1] + sims_5[draws_5[1],2]*logit(x) + sims_5[draws_5[1],3]*(0))
    , from = 0.5
    , to = 1
    , col = "black"
    , lwd=0.0       # make invisible, just used to set up plot. 
    , xlab="Adjusted Implied Probability of Favorite"
    , ylab = "Probability of Favorite Winning"
    , main = "Random Draws from Parameter Simulations"
    )

for (j in draws_5) {
  
   curve(invlogit(sims_5[j,1] + sims_5[j,2]*logit(x) + sims_5[j,3]*(-10))
        , col = "green"
        , lwd=0.3
        , add=TRUE
        )
  
    curve(invlogit(sims_5[j,1] + sims_5[j,2]*logit(x) + sims_5[j,3]*(-5))
        , col = "red"
        , lwd=0.3
        , add=TRUE
        )
  
    curve(invlogit(sims_5[j,1] + sims_5[j,2]*logit(x) + sims_5[j,3]*(0))
        , col = "black"
        , lwd=0.3
        , add=TRUE
        )
    
      curve(invlogit(sims_5[j,1] + sims_5[j,2]*logit(x) + sims_5[j,3]*(+5))
        , col = "blue"
        , lwd=0.3
        , add=TRUE
        )
      
      curve(invlogit(sims_5[j,1] + sims_5[j,2]*logit(x) + sims_5[j,3]*(+10))
        , col = "yellow"
        , lwd=0.3
        , add=TRUE
        )
}


abline(a=0, b=1, col = "black", lwd = 2, lty = 3)
```

Get point predictions and compare them to Adjusted Log Odds backtransformed to Implied Probabilities.    

```{r}
new_5 <- data.frame(Adjust_Log_Odds=rep(newx, each = 21), Delta_REACH_imp=newx_4)
new_5$Point_Pred <- predict(fit_5, type = "response", newdata = new_5)

new_5$BackTrans_Implied_Prob <- invlogit(new_5$Adjust_Log_Odds) 
```

Get expected outcome with uncertainty.  

```{r}
epred_5 <- posterior_epred(fit_5, newdata=new_5)

new_5$Means <- apply(epred_5, 2, mean)
new_5$SDs <- apply(epred_5, 2, sd)
```

Predictive distribution for new observation.  

```{r}
postpred_5 <- posterior_predict(fit_5, newdata=new_5)

new_5$Pred_Ms <- apply(postpred_5, 2, mean)
kable(new_5)
```

Calculate log score for model.   

```{r}
predp_5 <- predict(fit_5, type = "response")
logscore_5 <- sum(y * log(predp_5) + (1-y)*log(1-predp_5))
logscore_5

score_diff_5_m_1 <- round(logscore_5 - logscore_1,2)
```

The model's log score is only `r score_diff_5_m_1` points better than the model with just Adjusted Log Odds as a predictor. This advantage may go away with the LOO estimate.

Run leave one out cross validation.        

```{r}
loo_5 <- loo(fit_5)
print(loo_5)
```

The model with the Reach term (`r round(loo_5$estimates[[1,1]],2)`) is basically just as good as the model with just the Adjusted Log Odds (`r round(loo_1$estimates[[1,1]],2)`). Nonetheless, I will keep the Reach term since it makes sense a priori to have it in there. (Reach is typically considered an advantage in UFC fights by commentators etc.) 

<br>

### Plot Coefficients for Best Model  

The sign of the intercept is almost certainly negative.  

```{r}
plot(fit_5, plotfun = "areas", prob = 0.95,
     pars = "(Intercept)")
```

The Adjusted Log Odds coefficient is clearly positive. It is also almost certainly greater than 1, which means the slope is steeper that what you would expect if the Fight Odds perfectly tracked the Probability of the Favorite Winning. Indeed, this steep slope along with the negative intercept manifests in the clear trend whereby the Adjusted Odds overestimate mild Favorites but underestimate large ones.  

```{r}
plot(fit_5, plotfun = "areas", prob = 0.95,
     pars = "Adjust_Log_Odds")

```

```{r}
prob_reach_neg = round(mean(sims_5[, 3] < 0) * 100)
```


By analyzing the coefficient simulations, we can see that there is approximately a `r prob_reach_neg`% probability that the effect of Reach is negative.  

```{r}
plot(fit_5, plotfun = "areas", prob = 0.95,
     pars = "Delta_REACH_imp")
```

<br>

### Save Best Model

Save best model - the one with two predictors: Adjusted Log Odds and (imputed) Difference in Fighter Reach.  

```{r}
save(fit_5, file = "./Models/bayesian_logistic_regression_two_predictor_model.RData")
```

